{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt: \n",
    "### The company wants to predict customer retention. \n",
    "\n",
    "The data provided (data.csv) consists of a of a cohort of customer_ids with a first order_id (order_sequence = 1) in January 2014 and any subsequent order details. We consider a customer retained (order_sequence = 2) if they made a subsequent order from January 2014 to December 2014. \n",
    "Develop insights to find best predictors for retention and suggest ways to operationalize insights\n",
    "\n",
    "Description of data names and data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "order_date: date of the order placed\n",
    "category_name: category description (parent)\n",
    "product_name: product description (child)\n",
    "order_sequence: counter of the orders placed by the customer_id over lifetime\n",
    "revenue: revenue from the order\n",
    "units: units from the order\n",
    "customer_id: customer identifier\n",
    "order_id: order identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to answer with Data:\n",
    "\n",
    "1.\tBuild a predictive model to determine retention of a user. Discuss model approach, validity, and KPIs\n",
    "2.\tBriefly discuss how the compnay can leverage the insights gained from the model?\n",
    "\n",
    "My Answers: Data analysis below this cell helped me get to\n",
    "My answer to Q1: Summary: More info at bottom of notebook notebook. I chose logistic regression for speed even though the model performance is quite low.\n",
    "I choose a random forest due to possibility of improvements with a lot of parameter tuning later on.  Model performance is low, at with just under 60% accuracy (and no analysis done on whether retained or not retained customers are being more mis-classified). I would definitely need more time for parameter tuning and adjustment.\n",
    "\n",
    "My answer to Q2: Assuming there are no more variables for a customer, the model (with more improvements and tuning) can be used to automatically gauge whether a customer will come back. Itâ€™s possible that we should target a consumer with more promotions through email or through changes in website advertisements (in terms of frequency and/or depth) in order to ensure that they do come back and buy more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "random_seed_value = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Exploration\n",
    "Before I decide on a model, Let's look at the data format, shape, and summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv( 'data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_date</th>\n",
       "      <th>category_name</th>\n",
       "      <th>product_name</th>\n",
       "      <th>order_sequence</th>\n",
       "      <th>revenue</th>\n",
       "      <th>units</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-Jan-14</td>\n",
       "      <td>Gifts</td>\n",
       "      <td>Archive DVDs</td>\n",
       "      <td>1</td>\n",
       "      <td>17.48</td>\n",
       "      <td>1</td>\n",
       "      <td>34462512</td>\n",
       "      <td>3.450000e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6-Jan-14</td>\n",
       "      <td>Large Formats</td>\n",
       "      <td>11x14</td>\n",
       "      <td>1</td>\n",
       "      <td>38.34</td>\n",
       "      <td>6</td>\n",
       "      <td>75309835</td>\n",
       "      <td>7.530000e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Photo Books</td>\n",
       "      <td>12x12 Memory Book</td>\n",
       "      <td>1</td>\n",
       "      <td>54.54</td>\n",
       "      <td>1</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_date  category_name       product_name  order_sequence  revenue  \\\n",
       "0   2-Jan-14          Gifts       Archive DVDs               1    17.48   \n",
       "1   6-Jan-14  Large Formats              11x14               1    38.34   \n",
       "2   3-Jan-14    Photo Books  12x12 Memory Book               1    54.54   \n",
       "\n",
       "   units  customer_id      order_id  \n",
       "0      1     34462512  3.450000e+14  \n",
       "1      6     75309835  7.530000e+14  \n",
       "2      1     91660572  9.170000e+14  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89636, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_sequence</th>\n",
       "      <th>revenue</th>\n",
       "      <th>units</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>89636.000000</td>\n",
       "      <td>89636.000000</td>\n",
       "      <td>89636.000000</td>\n",
       "      <td>8.963600e+04</td>\n",
       "      <td>8.963600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.456123</td>\n",
       "      <td>19.523892</td>\n",
       "      <td>22.709637</td>\n",
       "      <td>4.951769e+07</td>\n",
       "      <td>4.528585e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.331564</td>\n",
       "      <td>35.721153</td>\n",
       "      <td>62.664103</td>\n",
       "      <td>2.879722e+07</td>\n",
       "      <td>3.075390e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-206.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.656000e+03</td>\n",
       "      <td>6.736700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.935735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.427273e+07</td>\n",
       "      <td>1.780000e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.990000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.898846e+07</td>\n",
       "      <td>4.440000e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>23.380000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7.419158e+07</td>\n",
       "      <td>7.170000e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>2761.990000</td>\n",
       "      <td>2974.000000</td>\n",
       "      <td>9.999470e+07</td>\n",
       "      <td>1.000000e+15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_sequence       revenue         units   customer_id      order_id\n",
       "count    89636.000000  89636.000000  89636.000000  8.963600e+04  8.963600e+04\n",
       "mean         4.456123     19.523892     22.709637  4.951769e+07  4.528585e+14\n",
       "std          6.331564     35.721153     62.664103  2.879722e+07  3.075390e+14\n",
       "min          1.000000   -206.150000      0.000000  8.656000e+03  6.736700e+04\n",
       "25%          1.000000      3.935735      1.000000  2.427273e+07  1.780000e+14\n",
       "50%          2.000000      7.990000      1.000000  4.898846e+07  4.440000e+14\n",
       "75%          5.000000     23.380000     13.000000  7.419158e+07  7.170000e+14\n",
       "max         93.000000   2761.990000   2974.000000  9.999470e+07  1.000000e+15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_sequence</th>\n",
       "      <th>revenue</th>\n",
       "      <th>units</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>order_sequence</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.073886</td>\n",
       "      <td>-0.027249</td>\n",
       "      <td>-0.016984</td>\n",
       "      <td>-0.053879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>-0.073886</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164429</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.034280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>units</th>\n",
       "      <td>-0.027249</td>\n",
       "      <td>0.164429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004011</td>\n",
       "      <td>-0.048096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_id</th>\n",
       "      <td>-0.016984</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>-0.004011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.851578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order_id</th>\n",
       "      <td>-0.053879</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>0.851578</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                order_sequence   revenue     units  customer_id  order_id\n",
       "order_sequence        1.000000 -0.073886 -0.027249    -0.016984 -0.053879\n",
       "revenue              -0.073886  1.000000  0.164429     0.002186  0.034280\n",
       "units                -0.027249  0.164429  1.000000    -0.004011 -0.048096\n",
       "customer_id          -0.016984  0.002186 -0.004011     1.000000  0.851578\n",
       "order_id             -0.053879  0.034280 -0.048096     0.851578  1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we have no abnormal data issues- nas and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_date        0\n",
       "category_name     0\n",
       "product_name      0\n",
       "order_sequence    0\n",
       "revenue           0\n",
       "units             0\n",
       "customer_id       0\n",
       "order_id          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_date         object\n",
       "category_name      object\n",
       "product_name       object\n",
       "order_sequence      int64\n",
       "revenue           float64\n",
       "units               int64\n",
       "customer_id         int64\n",
       "order_id          float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking relatively clean so far! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove orders in 2015, since not relevant to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.loc[df['order_date'].str.contains(\"14\"), ].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an indicator for a customer who first ordered in january."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['jan14_order'] = df['order_date'].str.contains(\"Jan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an indicator for first orders in january 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['firstorder_jan14'] = ( df['jan14_order'] &  ( df['order_sequence'] == 1) )\n",
    "customers_first_order_jan = df.loc[ df['firstorder_jan14'] == True, 'customer_id' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31840"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(customers_first_order_jan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22036"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(customers_first_order_jan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, customers order multiple items per order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df.customer_id) - set(customers_first_order_jan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# looks like every customer in the list first ordered in january"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_date          0\n",
       "category_name       0\n",
       "product_name        0\n",
       "order_sequence      0\n",
       "revenue             0\n",
       "units               0\n",
       "customer_id         0\n",
       "order_id            0\n",
       "jan14_order         0\n",
       "firstorder_jan14    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the orders for one customer who, which I found when scrolling through the dataset in excel, bought multiple items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_date</th>\n",
       "      <th>category_name</th>\n",
       "      <th>product_name</th>\n",
       "      <th>order_sequence</th>\n",
       "      <th>revenue</th>\n",
       "      <th>units</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>jan14_order</th>\n",
       "      <th>firstorder_jan14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Photo Books</td>\n",
       "      <td>12x12 Memory Book</td>\n",
       "      <td>1</td>\n",
       "      <td>54.540000</td>\n",
       "      <td>1</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Photo Books</td>\n",
       "      <td>Memorabilia Pocket</td>\n",
       "      <td>1</td>\n",
       "      <td>1.990000</td>\n",
       "      <td>0</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Photo Books</td>\n",
       "      <td>Premium Content</td>\n",
       "      <td>1</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Prints</td>\n",
       "      <td>4x6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.691361</td>\n",
       "      <td>23</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Prints</td>\n",
       "      <td>5x7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.910607</td>\n",
       "      <td>1</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3-Jan-14</td>\n",
       "      <td>Prints</td>\n",
       "      <td>8x10</td>\n",
       "      <td>1</td>\n",
       "      <td>14.708032</td>\n",
       "      <td>4</td>\n",
       "      <td>91660572</td>\n",
       "      <td>9.170000e+14</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_date category_name        product_name  order_sequence    revenue  \\\n",
       "2   3-Jan-14   Photo Books   12x12 Memory Book               1  54.540000   \n",
       "3   3-Jan-14   Photo Books  Memorabilia Pocket               1   1.990000   \n",
       "4   3-Jan-14   Photo Books     Premium Content               1   4.990000   \n",
       "5   3-Jan-14        Prints                 4x6               1   0.691361   \n",
       "6   3-Jan-14        Prints                 5x7               1   0.910607   \n",
       "7   3-Jan-14        Prints                8x10               1  14.708032   \n",
       "\n",
       "   units  customer_id      order_id  jan14_order  firstorder_jan14  \n",
       "2      1     91660572  9.170000e+14         True              True  \n",
       "3      0     91660572  9.170000e+14         True              True  \n",
       "4      0     91660572  9.170000e+14         True              True  \n",
       "5     23     91660572  9.170000e+14         True              True  \n",
       "6      1     91660572  9.170000e+14         True              True  \n",
       "7      4     91660572  9.170000e+14         True              True  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[ df['customer_id'] == 91660572, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder why units of the photo book, premium content are 0. It says [here](https://support.shutterfly.com/s/article/Photo-Books-Memorabilia-Pocket-1) that they are add on items for the memory book. I definitely appreciate that units is not 1 then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an indicator for those who are retained. First let's find the max order value for each customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_cust_order_sequence =  df.groupby( 'customer_id', as_index = True).max()['order_sequence']\n",
    "max_cust_order_sequence = max_cust_order_sequence.reset_index( drop = False)\n",
    "max_cust_order_sequence.columns = ['customer_id', 'max_order_sequence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the average number of orders across all customers? Let's get the maximum order sequence of each customer to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22036.000000\n",
       "mean         2.032991\n",
       "std          2.361412\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          2.000000\n",
       "max         61.000000\n",
       "Name: order_sequence, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('customer_id', as_index = True)['order_sequence'].max().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 50% of consumers don't come back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge( df, max_cust_order_sequence, how ='left', on = 'customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if the max number of the order sequence of a customer is > 1, then they were retained\n",
    "# OR were a customer already in the dataset previously\n",
    "df['retained'] = (df['max_order_sequence'] > 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     44401\n",
       "False    18911\n",
       "Name: retained, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['retained'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3745"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.order_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we continue to explore the dataset to figure out the model, Let's look at the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prints           21532\n",
       "Photo Books      16981\n",
       "Cards             6574\n",
       "Gifts             6048\n",
       "Calendars         4280\n",
       "Home Decor        2577\n",
       "Card Upsell       1706\n",
       "Large Formats     1670\n",
       "Stationery        1509\n",
       "Shipping           189\n",
       "Services           107\n",
       "Unassigned          68\n",
       "Other               63\n",
       "Yearbooks            6\n",
       "OTHER                2\n",
       "Name: category_name, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4x6                           13680\n",
       "8x8 Story Book                 6003\n",
       "5x7                            3968\n",
       "Premium Content                3909\n",
       "8x11 Classic Book              3722\n",
       "Wall Calendars                 3079\n",
       "8x10                           2664\n",
       "5x7 Flat Card (Premium)        2373\n",
       "Magnets                        1812\n",
       "Mugs                           1389\n",
       "Address Labels                 1349\n",
       "Card Trims                     1335\n",
       "10x10 Photo Book                973\n",
       "Greeting Cards                  901\n",
       "Large Calendars                 815\n",
       "12x12 Memory Book               796\n",
       "4x8s                            792\n",
       "Memorabilia Pocket              779\n",
       "Mousepads                       753\n",
       "11x14                           727\n",
       "16x20                           683\n",
       "Wallet                          642\n",
       "Desk Art                        616\n",
       "5x7s                            573\n",
       "iPhone Case                     523\n",
       "6x8 Stationery Card             470\n",
       "3x5 Stationery Card             415\n",
       "11x14 Photo Book                414\n",
       "Pearl Paper                     371\n",
       "4.25x5.5 Stationery Cards       365\n",
       "                              ...  \n",
       "Gift Tags                         7\n",
       "Photo Quilt                       7\n",
       "Decorative Wall Decals            6\n",
       "Notepad                           6\n",
       "Stamps                            6\n",
       "Candle                            6\n",
       "6x8 Flat Foil Card                6\n",
       "iPad Sleeve                       6\n",
       "Envelopes                         5\n",
       "Dimensional Wall Art              5\n",
       "Stick It Notes                    5\n",
       "Lunch Bags                        5\n",
       "Growth Chart                      4\n",
       "Electronic Accessories            4\n",
       "Acrylic Photo Blocks - 5X5        3\n",
       "Pet Tag                           3\n",
       "Ornament Cards                    3\n",
       "PhotoShow DVDs                    2\n",
       "Pre-Add Envelopes To              2\n",
       "Aprons                            2\n",
       "Wedding Invites                   2\n",
       "OTHER                             2\n",
       "Charm                             2\n",
       "Totebags                          2\n",
       "Gift Wrap                         2\n",
       "Art Prints                        1\n",
       "Framed Art Prints                 1\n",
       "Framed Mounted Wall Art           1\n",
       "4X4                               1\n",
       "11x14 MIAM                        1\n",
       "Name: product_name, Length: 141, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.product_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While product and category were intially items that I was just going to put into the model, there clearly are a lot of combinations of both! Maybe not a good idea to just throw this column and the category column into pd.dummies and interpret results out of a regression. \n",
    "\n",
    "Since interpretation of predictors is a priority, if we do end up doing a logit or decision tree (or another interpretative model) due to the small sample sizes of shipping, services, other, OTHER, yearbooks, I wouldn't trust those values. I would combine these groups into larger groups if I had time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features in Final Model:\n",
    "I will use a logistic regression and random forest due to easy of interpretability of the results. \n",
    "\n",
    "Features I'll put into the algorithm are:\n",
    "* total revenue for customer in first sale - maybe users who who spend a certain amount are more likely to come back (not\n",
    "* num of units per category type for customer in first sale - maybe users who buy certain categories are more likely to come back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's subset the data on the first order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_order1 = df.loc[df['order_sequence'] == 1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no minority class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_order1['total_revenue'] = df_order1.groupby('customer_id', sort=False)['revenue'].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31840, 13)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_order1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate columns that will be the amount of units a customer bought in their first January order in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_by_category = df_order1.groupby( ['customer_id', 'category_name'], as_index = True).sum()['units'].reset_index( drop = False)\n",
    "sum_by_category = sum_by_category.pivot(index='customer_id', columns='category_name', values='units')\n",
    "# fill na with 0, since that means that the customer didn't buy anything in their first order\n",
    "sum_by_category[ sum_by_category.isna() ] = 0\n",
    "sum_by_category = sum_by_category.reset_index( drop = False )\n",
    "#sum_by_category = sum_by_category.drop(1, axis= 0) #prevent possible multi collinearity with total revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_name\n",
       "customer_id     -2147483648\n",
       "Calendars              4310\n",
       "Card Upsell               0\n",
       "Cards                162443\n",
       "Gifts                  3466\n",
       "Home Decor             1438\n",
       "Large Formats          1113\n",
       "Other                     0\n",
       "Photo Books            7779\n",
       "Prints               593310\n",
       "Services               3682\n",
       "Shipping                  0\n",
       "Stationery              886\n",
       "Yearbooks                41\n",
       "dtype: int32"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_by_category.sum().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some variables sum up to close to zero, making a matrix that isn't invertible. let's drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_by_category= sum_by_category.drop(['Other', 'Card Upsell', 'Shipping', 'Yearbooks'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now manipulate the original dataframe of first orders so it's at the customer level. Then merge it to sum by category dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_order2 = df_order1.loc[  ~df_order1.customer_id.duplicated(), ]\n",
    "df_order2 = df_order2[['customer_id', 'total_revenue', 'retained']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't expect a ton of multi collinearity between revenue and category due to variation in revenue likely due to sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final = pd.merge( sum_by_category.reset_index(drop = True), df_order2, how = 'left', on='customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df_final.drop(['retained', 'customer_id'] , axis = 1).reset_index(drop = True)\n",
    "y = df_final['retained'].reset_index( drop = True)\n",
    "\n",
    "x = x.reset_index( drop = True)\n",
    "x = sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'd like to run the tool on the original Logit function in order to make it easier to interpret coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.672764\n",
      "         Iterations 5\n"
     ]
    }
   ],
   "source": [
    "logit_noreg= sm.Logit(y, x).fit()\n",
    "logit1_res = logit_noreg.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>retained</td>     <th>  No. Observations:  </th>  <td> 22036</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 22025</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    10</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 28 Jun 2019</td> <th>  Pseudo R-squ.:     </th> <td>0.0004660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:50:58</td>     <th>  Log-Likelihood:    </th> <td> -14825.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -14832.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td>0.1812</td>  \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>         <td>   -0.4193</td> <td>    0.019</td> <td>  -22.552</td> <td> 0.000</td> <td>   -0.456</td> <td>   -0.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Calendars</th>     <td>    0.0053</td> <td>    0.010</td> <td>    0.550</td> <td> 0.582</td> <td>   -0.014</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Cards</th>         <td>    0.0002</td> <td>    0.001</td> <td>    0.218</td> <td> 0.827</td> <td>   -0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gifts</th>         <td>   -0.0027</td> <td>    0.018</td> <td>   -0.148</td> <td> 0.882</td> <td>   -0.038</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Home Decor</th>    <td>   -0.0021</td> <td>    0.040</td> <td>   -0.052</td> <td> 0.959</td> <td>   -0.081</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Large Formats</th> <td>   -0.0388</td> <td>    0.037</td> <td>   -1.059</td> <td> 0.290</td> <td>   -0.111</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Photo Books</th>   <td>    0.0188</td> <td>    0.027</td> <td>    0.705</td> <td> 0.481</td> <td>   -0.033</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prints</th>        <td>    0.0005</td> <td>    0.000</td> <td>    2.748</td> <td> 0.006</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Services</th>      <td>   -0.0054</td> <td>    0.004</td> <td>   -1.411</td> <td> 0.158</td> <td>   -0.013</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Stationery</th>    <td>   -0.0356</td> <td>    0.036</td> <td>   -1.002</td> <td> 0.317</td> <td>   -0.105</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_revenue</th> <td>   -0.0001</td> <td>    0.001</td> <td>   -0.238</td> <td> 0.812</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               retained   No. Observations:                22036\n",
       "Model:                          Logit   Df Residuals:                    22025\n",
       "Method:                           MLE   Df Model:                           10\n",
       "Date:                Fri, 28 Jun 2019   Pseudo R-squ.:               0.0004660\n",
       "Time:                        16:50:58   Log-Likelihood:                -14825.\n",
       "converged:                       True   LL-Null:                       -14832.\n",
       "                                        LLR p-value:                    0.1812\n",
       "=================================================================================\n",
       "                    coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------\n",
       "const            -0.4193      0.019    -22.552      0.000      -0.456      -0.383\n",
       "Calendars         0.0053      0.010      0.550      0.582      -0.014       0.024\n",
       "Cards             0.0002      0.001      0.218      0.827      -0.001       0.002\n",
       "Gifts            -0.0027      0.018     -0.148      0.882      -0.038       0.033\n",
       "Home Decor       -0.0021      0.040     -0.052      0.959      -0.081       0.077\n",
       "Large Formats    -0.0388      0.037     -1.059      0.290      -0.111       0.033\n",
       "Photo Books       0.0188      0.027      0.705      0.481      -0.033       0.071\n",
       "Prints            0.0005      0.000      2.748      0.006       0.000       0.001\n",
       "Services         -0.0054      0.004     -1.411      0.158      -0.013       0.002\n",
       "Stationery       -0.0356      0.036     -1.002      0.317      -0.105       0.034\n",
       "total_revenue    -0.0001      0.001     -0.238      0.812      -0.001       0.001\n",
       "=================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_noreg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit_to_prob(logit_results):\n",
    "    #Takes a coefficient from the logit model and returns the hazard estimate.\n",
    "    return np.exp(logit_results)/(1 + np.exp(logit_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const            0.396692\n",
       "Calendars        0.501318\n",
       "Cards            0.500042\n",
       "Gifts            0.499335\n",
       "Home Decor       0.499481\n",
       "Large Formats    0.490308\n",
       "Photo Books      0.504692\n",
       "Prints           0.500130\n",
       "Services         0.498646\n",
       "Stationery       0.491093\n",
       "total_revenue    0.499970\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit1_res.apply(logit_to_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the unregularized logistic fit on all the data, probabilities of customer being retained returning are pretty similar across all variables. We of course don't have interaction terms, and that may help enrich how to interpret these results. The pseduo r^2 is very low which doesn't bode well for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a an algorithm that's just focused on prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize the independent variables\n",
    "stsca = StandardScaler()\n",
    "stsca.fit(x)\n",
    "x_norm = stsca.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinda small dataframe so let's say 20000/5 = 40000 - 5 folds should be okay to have a lot of training datasets. With more time I would tune regularlization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold( n_splits=5, shuffle=True, random_state=random_seed_value)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',max_iter = 1000,  multi_class='multinomial')\n",
    "scores_logit = cross_val_score(clf, x, y, cv=kf, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.599337189185494"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "kf_rf = KFold( n_splits=5, shuffle=True, random_state=random_seed_value)\n",
    "clf_rf =  RandomForestClassifier()\n",
    "scores_rf = cross_val_score(clf_rf, x, y, cv=kf_rf, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5544567255442895"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With more time I would do the following:\n",
    "\n",
    "1. Add more variables\n",
    "    1. Due to large amount of variables in 'product_type' variable, and possible multi-collinearity with it and 'category_type', I'm not using it in the interpretation regression. I would definitely use a combination of text analysis, knowledge of the company's items, as well as knowledge of s's consumer base to see if I can regroup the category and product columns into more higher level groups/characteristics. Ultimately having higher level groups can help the regressions have more data on what types of items consumers are buying as well as produce interpretable coeffs that can be useful when deciding which categories/product types to focus on. Whether this is actually feasible would definitely depend on the investment the company has in this level of user research. These are some ideas for groups we can split items into for analysis.\n",
    "        * size of product - maybe larger size items are correlated with a more 'unique' item - then would be a 1 time purchaser. This would also depend on the category we are in. Maybe someone is more likely to buy a lot of large size photos vs not as likely to buy a lot of large pillows\n",
    "        * product function (a novelty item like a an apron vs photo books - there may be different levels of novelty items as well.)\n",
    "    2. Look into using order date variable for results. Maybe weekend users are different are different from weekday users/certain promos happen at certain times?\n",
    "    3. Convert the # units of product/category into a percentage. Didn't do this to time constraints but there is likely high multi collinearity with revenue now which will affect the logistic regression interpretability and standard errors - it's possible that the effect sizes we're seeing are 0 with too high standard errors.   \n",
    "2. Check outliers for each feature. For example, I'm sure there are a few anomaly users who could 'bias' our results and reduce extrapolation of this model to an average user. If they would end up coming anyway, it may not be worth adjusting marketing strategy to them. This would require a lot of consumer research. There are also maybe some categories that don't have a lot of sales, so we should take the interpretation of the coefficients of those categories with a grain of salt due to low sample size.\n",
    "3. Consider other alogrithms for prediction. Naive bayes is an obvious alternative to logistic as it can give probabilities (sacrificing obvious dependence between variables, and maybe increasing bias). Neural networks are other alternatives that come to mind but sacrifice interpretability of model results (and can also overfit if not careful)\n",
    "4. Parameter tuning (particularly of regularization parameter in logit; depth of branches, min splits, minimum number of samples for random forest leaves using random search\n",
    "\n",
    "It would also be interesting to\n",
    "* It would be to cut the data and see if certain factors effect customer retention beyond order 2 and order 3. We would be able to use information about subsequent orders then. If we had prior data, we could operationalize the dataset beyond these customers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
